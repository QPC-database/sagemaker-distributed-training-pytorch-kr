{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EIA를 이용한 Torchscript Inference\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API에서 Amazon Elastic Inference Accelerator는를 이용하여 CPU에서 cost-optimized하게 inference 를 수행해 봅니다.\n",
    "<p><img src=\"./imgs/eia.png\" width=\"500\", height=\"50\"></p>\n",
    "Amazon Elastic Inference Accelerator는 GPU 기반 추론 가속화의 올바른 양을 Amazon EC2, Amazon SageMaker 인스턴스 유형 또는 Amazon ECS 작업에 연결할 수 있습니다. 따라서 이제 애플리케이션의 전체 컴퓨팅, 메모리 및 스토리지 요구 사항에 가장 적합한 인스턴스 유형을 선택한 후 필요한 양의 추론 가속을 별도로 구성할 수 있습니다.\n",
    "저렴한 비용으로 딥 러닝 학습 추론 작업량을 가속화하기 위해 모든 EC2 인스턴스, SageMaker 인스턴스 또는 ECS 작업과 함께 작동하도록 설계된 GPU 기반 하드웨어 장치입니다. Amazon Elastic Inference를 사용하여 EC2 인스턴스 또는 ECS 작업을 시작하면 액셀러레이터가 네트워크를 통해 인스턴스에 프로비저닝되고 연결됩니다. Amazon Elastic Inference에 사용할 수 있는 TensorFlow Serving, Apache MXNet 및 PyTorch와 같은 딥 러닝 도구 및 프레임워크는 연결된 엑셀러레이터에 모델 계산을 자동으로 감지하고 오프로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "    !{sys.executable} -m pip install https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp36-cp36m-linux_x86_64.whl --use-feature=2020-resolver\n",
    "    !{sys.executable} -m pip install https://s3.amazonaws.com/amazonei-pytorch/torch_eia-1.3.1-cp36-cp36m-manylinux1_x86_64.whl --use-feature=2020-resolver\n",
    "\n",
    "    !{sys.executable} -m pip install graphviz==0.13.2   \n",
    "    !{sys.executable} -m pip install mxnet-model-server==1.0.8\n",
    "    !{sys.executable} -m pip install pillow==7.1.0\n",
    "    !{sys.executable} -m pip install sagemaker_containers\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import predictor\n",
    "\n",
    "import src_dir.util as util\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import subprocess\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Trace_model 생성\n",
    "Pytorch의 경우 Elastic Inference에서 사용하기 위해서는 [TorchScript](https://pytorch.org/docs/1.3.1/jit.html)로 모델을 compile 해야 합니다. TorchScript로 compile하는 방법은 tracing과 scripting 2가지 방법을 사용하여 compile이 가능하며, 두 경우 모두 computation graph를 생성하게 됩니다. \n",
    "\n",
    "scripting 방식은 Torchscript를 compile하는데 선호하는 방식 중 하나이며, 모든 모델의 로직이 유지됩니다. 하지만, scripted할 수 있는 모델들이 tracing 방식보다 작기 때문에 이 점을 고려해서 이 모듈에서는 tracing 방식을 사용할 예정입니다. 모델에 따라서는 두 방식 모두 TorchScript에 compatible하지 않을 수 있으며 그럴 경우 모델의 수정이 필요할 수 있습니다.\n",
    "\n",
    "EIA는 현재 Pytorch 1.3.1에서 지원이 가능하며, control-flow operations를 다루기 때문에 많이 conditional branch가 포함된 모델을 scripting 방식으로 수행하게 되면 inference의 latency가 발생할 수 있습니다. 또한, Pytorch 1.3.1에서는 scripting 방식보다는 tracing 방식이 더 좋다는 의견이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = util.torch_model(hyperparameters['model_name'],num_classes=hyperparameters['num-classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multigpus_distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multigpus_distributed\n",
    "if multigpus_distributed:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "with open(os.path.join(model_dir, 'model_best.pth'), 'rb') as f:\n",
    "    model_load = torch.load(f, map_location=torch.device('cpu'))\n",
    "    class_to_idx = model_load['class_to_idx']\n",
    "    state_dict = model_load['state_dict']\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_dir ='./model_result'\n",
    "!rm -rf $model_out_dir\n",
    "!mkdir $model_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing 방식은 특정 input을 모델에 실행했을 때 수행되면서 operation이 저장하기 때문에, random으로된 input을 수행하여 operation을 record합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(model_out_dir, \"model.pt\")\n",
    "cv_input = torch.rand(1,3,hyperparameters['width'],hyperparameters['height'])\n",
    "model = torch.jit.trace(model.eval(), cv_input)\n",
    "torch.jit.save(model, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.load(os.path.join(model_out_dir, 'model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Test_Traced_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(filename):\n",
    "    im = Image.open(filename)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((hyperparameters['width'],hyperparameters['height'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    img_data = transform(im)\n",
    "    return img_data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {}\n",
    "for class_name in class_to_idx:\n",
    "    idx_to_class[class_to_idx[class_name]] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_path = './dataset/val/*/*'\n",
    "filename = glob.glob(val_img_path)[0]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = get_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.jit.optimized_execution블록은 Elatic Inference에서 traced model을 사용하기 위해 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True, {'target_device': 'eia:0'}):\n",
    "  # You can trace with any input\n",
    "    output = traced_model(im)\n",
    "\n",
    "res_predict = output.max(1, keepdim=True)[1].numpy()[0][0]\n",
    "print(idx_to_class[res_predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. writing Inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src_dir/inference.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import copy\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    This function is called by the Pytorch container during hosting when running on SageMaker with\n",
    "    values populated by the hosting environment.\n",
    "\n",
    "    This function loads models written during training into `model_dir`.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"****model_dir : {}\".format(model_dir))\n",
    "    traced_model = torch.jit.load(os.path.join(model_dir, 'model_result/model.pt'))\n",
    "    return traced_model\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type='application/x-image'):\n",
    "    \"\"\"This function is called on the byte stream sent by the client, and is used to deserialize the\n",
    "    bytes into a Python object suitable for inference by predict_fn .\n",
    "    \"\"\"\n",
    "    logger.info('An input_fn that loads a image tensor')\n",
    "    if request_content_type == 'application/x-image':\n",
    "        img = Image.open(io.BytesIO(request_body))\n",
    "#         img_arr = np.array(Image.open(io.BytesIO(request_body)))\n",
    "#         img = Image.fromarray(img_arr.astype('uint8')).convert('RGB')\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        img_data = transform(img)\n",
    "        return img_data\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Requested unsupported ContentType in content_type : ' + request_content_type)\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    This function receives a NumPy array and makes a prediction on it using the model returned\n",
    "    by `model_fn`.\n",
    "    \"\"\"\n",
    "    logger.info('Entering the predict_fn function')\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    result = {}\n",
    "    with torch.no_grad():\n",
    "        with torch.jit.optimized_execution(True, {\"target_device\": \"eia:0\"}):\n",
    "            output = model(input_data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            result['output'] = output.numpy()\n",
    "            result['pred'] = pred\n",
    "    return result        \n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\n",
    "    \"\"\"This function is called on the return value of predict_fn, and is used to serialize the\n",
    "    predictions back to the client.\n",
    "    \"\"\"\n",
    "    return json.dumps({'result': prediction_output['output'].tolist(), 'pred': prediction_output['pred'].tolist()}), accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JIT 이용한 Model_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_filename = 'test_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprocess.call(['tar', '-czvf', tar_filename, os.path.join(model_out_dir, 'model.pt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $tar_filename $artifacts_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session().client(service_name='sagemaker')\n",
    "list_endpoints = sm.list_endpoints()\n",
    "\n",
    "for ep in list_endpoints['Endpoints']:\n",
    "    sm.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "    \n",
    "\n",
    "NextToken = 'None'\n",
    "while NextToken !='':\n",
    "    lec = sm.list_endpoint_configs(NextToken=NextToken) if NextToken != 'None' else sm.list_endpoint_configs()\n",
    "    for epc in lec['EndpointConfigs']:\n",
    "        print(epc['EndpointConfigName'])\n",
    "        sm.delete_endpoint_config(EndpointConfigName=epc['EndpointConfigName'])\n",
    "        time.sleep(3)\n",
    "    NextToken = lec['NextToken'] if lec.get('NextToken') else ''\n",
    "\n",
    "NextToken = 'None'\n",
    "while NextToken !='':\n",
    "    lec = sm.list_models(NextToken=NextToken) if NextToken != 'None' else sm.list_models()\n",
    "    for epc in lec['Models']:\n",
    "        print(epc['ModelName'])\n",
    "        sm.delete_model(ModelName=epc['ModelName'])\n",
    "        time.sleep(3)\n",
    "    NextToken = lec['NextToken'] if lec.get('NextToken') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"endpoint-pet-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=artifacts_dir + 'test_model.tar.gz', \n",
    "                                   role=role,\n",
    "                                   entry_point='./src_dir/inference.py',\n",
    "                                   framework_version='1.3.1', \n",
    "                                   py_version='py3',\n",
    "                                   container_log_level='error')\n",
    "\n",
    "pytorch_model.deploy(instance_type='ml.c5.large', \n",
    "                     initial_instance_count=1, \n",
    "                     accelerator_type='ml.eia2.large', \n",
    "                     endpoint_name=endpoint_name,\n",
    "                     wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 결과 확인 (Locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러분의 개인 랩탑/데스크탑이나 온프레미스에서 수행하는 방법과 동일하게 배치 데이터도 쉽게 추론이 가능합니다.  \n",
    "본 예시에서는 테스트 데이터에 대해서 간단하게 배치 추론을 수행해 보고, 기본적인 평가 지표들인 ```Confusion Matrix```, ```AUROC(Area Under a ROC Curve)```, ```AUPRC(Area Under a Precision-Recall Curve)```를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from util import inference_utils as iu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = './dataset/test/*/*'\n",
    "test_img_list = glob.glob(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_str = [img_list.split('/')[3] for img_list in test_img_list]\n",
    "y_true = np.array([class_to_idx[s] for s in y_true_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_score = []\n",
    "y_pred1 = []\n",
    "y_score1 = []\n",
    "y_revised_true = []\n",
    "with torch.jit.optimized_execution(True, {'target_device': 'eia:0'}):\n",
    "    for i, test_img in enumerate(test_img_list):\n",
    "        try:\n",
    "            im = get_image(test_img)\n",
    "            output = traced_model(im)\n",
    "            output = output[:, 0:37]\n",
    "            softmax = np.exp(output)/np.exp(output).sum()\n",
    "            result = softmax.max(1, keepdim=True)\n",
    "            indices = result[1].numpy()[0][0]\n",
    "            y_score.extend(softmax.numpy())\n",
    "            y_pred.append(indices)\n",
    "            y_revised_true.append(y_true[i])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = np.array(y_score)\n",
    "y_pred = np.array(y_pred)\n",
    "y_revised_true = np.array(y_revised_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_score.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 클래스의 plotting에 필요한 컬러 테이블을 랜덤하게 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "color_table = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(len(class_to_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list(class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.plot_conf_mtx_multiclass(y_revised_true, y_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories='auto', sparse=False)\n",
    "num_classes = len(labels)\n",
    "\n",
    "y_true_ohe = enc.fit_transform(y_revised_true.reshape(-1, 1))\n",
    "y_pred_ohe = enc.fit_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "y_pred_str = [idx_to_class[int(score)] for score in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = pd.DataFrame(y_score)\n",
    "y_score = y_score.fillna(0)\n",
    "y_score = y_score.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.plot_roc_curve_multiclass(y_true_ohe, y_score, num_classes, color_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu.plot_pr_curve_multiclass(y_true_ohe, y_score, num_classes, color_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 결과 확인 (SageMaker Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, mode='rb') as file:\n",
    "    img = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.Predictor.accept = 'application/json'\n",
    "predictor.Predictor.content_type = 'application/x-image'\n",
    "\n",
    "t_pred = []\n",
    "t_gt = []\n",
    "\n",
    "response = predictor.Predictor(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit response.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
